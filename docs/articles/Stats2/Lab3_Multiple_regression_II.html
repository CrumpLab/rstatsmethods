<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Lab 3 Multiple Regression II • psyc7709Lab</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><meta property="og:title" content="Lab 3 Multiple Regression II">
<meta property="og:description" content="psyc7709Lab">
<meta property="og:image" content="https://crumplab.github.io/psyc7709Lab/logo.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@MattCrump_">
<meta name="twitter:site" content="@MattCrump_">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-79429674-12"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-79429674-12');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">psyc7709Lab</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased version">0.0.0.9004</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Syllabus
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/course_docs/Spring_syllabus.html">Spring 2021</a>
    </li>
    <li>
      <a href="../../articles/course_docs/Syllabus.html">Fall 2020</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">STATS 2 Spring 2021</li>
    <li>
      <a href="../../articles/Stats2/Lab1_Shape.html">Lab 1 Shaping Data</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab2_Multiple_regression_I.html">Lab 2 Multiple Regression</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab3_Multiple_regression_II.html">Lab 3 Multiple Regression II</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab4_ANOVA.html">Lab 4 ANOVA</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab5_ANOVA_regression.html">Lab 5 ANOVA as Regression</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab6_Comparisons.html">Lab 6 Comparisons</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab7_Factorial_ANOVA.html">Lab 7 Factorial ANOVA</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab8_Contrasts.html">Lab 8 Contrast Analyses</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab9_RM_ANOVA.html">Lab 9 RM ANOVA</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab10_Nested_Designs.html">Lab 10 Nested Designs</a>
    </li>
    <li>
      <a href="../../articles/Stats2/Lab11_WYOR.html">Lab 11 WYOR</a>
    </li>
    <li class="dropdown-header">STATS 1 FALL 2020</li>
    <li>
      <a href="../../articles/Stats1/Getting_Started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab1_Basics.html">Lab 1 Basics</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab2_Descriptives.html">Lab 2 Descriptives</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab3_Distributions_I.html">Lab 3 Distributions I</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab4_Distributions_II.html">Lab 4 Distributions II</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab5_Sampling_Distributions.html">Lab 5 Sampling Distributions</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab6_Statistical_Inference.html">Lab 6 Statistical Inference</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab7_Binomial.html">Lab 7 Binomial tests</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab8_Normal.html">Lab 8 Z tests</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab9_Chisquare.html">Lab 9 Chi Square</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab10_ttest.html">Lab 10 t tests</a>
    </li>
    <li>
      <a href="../../articles/Stats1/SP_Power.html">Simulation and Power analysis I</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab11_Correlation.html">Lab 11 Correlation</a>
    </li>
    <li>
      <a href="../../articles/Stats1/Lab12_Regression.html">Lab 12 Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/course_docs/reference_code.html">Coding Reference</a>
    </li>
    <li>
      <a href="../../articles/course_docs/resources.html">More resources</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Semester Project
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/Stats2/semester_project.html">Spring 2021</a>
    </li>
    <li>
      <a href="../../articles/Stats1/semester_project.html">Fall 2020</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/course_docs/contribute.html">Extra Credit</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/CrumpLab/psyc7709Lab">
    <span class="fas fa-lg fa-github"></span>
     
  </a>
</li>
<li>
  <a href="../../news/index.html">news</a>
</li>
<li>
  <a href="../../reference/index.html"></a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="Lab3_Multiple_regression_II_files/header-attrs-2.6/header-attrs.js"></script><script src="Lab3_Multiple_regression_II_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Lab 3 Multiple Regression II</h1>
                        <h4 class="author">Matthew J. C. Crump</h4>
            
            <h4 class="date">2/17/2021</h4>
      
      
      <div class="hidden name"><code>Lab3_Multiple_regression_II.Rmd</code></div>

    </div>

    
    
<div id="readings" class="section level2">
<h2 class="hasAnchor">
<a href="#readings" class="anchor"></a>Readings</h2>
<p>Chapters 6 on semi-partial correlation from <span class="citation">Abdi et al. (<a href="#ref-abdiExperimentalDesignAnalysis2009" role="doc-biblioref">2009</a>)</span>.</p>
</div>
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p>An overarching goal of this semester is to discuss how experimental research designs and statistical tools are combined together to test causal hypotheses about psychological phenomena. For the most part, we will use ANOVA as the statistical tool for experimental research designs with multiple IVs. Currently, we are discussing regression en route to discussing ANOVA. One reason is that ANOVA and regression are fundamentally the same analyses, and it is important to recognize that equivalence. A focus of today’s lab is the concept of semi-partial correlation in the context of non-orthogonal multiple regression. This is a statistical tool that is often is used in non-experimental research, and it is useful to discuss some its uses and limitations as we prepare for ANOVA next week.</p>
</div>
<div id="a-note-on-explanation" class="section level2">
<h2 class="hasAnchor">
<a href="#a-note-on-explanation" class="anchor"></a>A note on Explanation</h2>
<p>Briefly, we will be using the word explanation many times throughout this lab and the next ones. For the most part we will be talking about explanation in a highly restricted statistical sense. For example, <span class="math inline">\(R^2\)</span>, termed the co-efficient of determination, is often described as a quantity of explanation; specifically, how much variation in one variable <strong>explains</strong> variation in another. <span class="math inline">\(F\)</span> is a ratio of “explained” variance divided by “unexplained” variance. Because we will mainly deal with linear models, “explanation” always refers to a geometrical account of the the data, wherein data points are described as one part that falls on a line (the explained part), and a leftover part that does not (error, or unexplained part). Although we use the word explanation, this term does not imply anything about theoretical or causal explanation.</p>
</div>
<div id="concept-i-explaining-variance-with-multiple-variables" class="section level2">
<h2 class="hasAnchor">
<a href="#concept-i-explaining-variance-with-multiple-variables" class="anchor"></a>Concept I: Explaining variance with multiple variables</h2>
<p>In multiple linear regression there is one dependent variable and multiple predictor variables. The variation in the dependent variable is “explained” in terms of combinations of linear relationships to the other variables.</p>
<p>One issue with multiple linear regression is that adding more predictor variables generally increases the amount of variation explained. We quickly illustrate this below:</p>
<p>First, I create a matrix of random values from a normal distribution. There are 26 variables, each labeled from a to z. We will pick <code>a</code> as the DV, and use the other random vectors as predictor variables to explain the variation in <code>a</code></p>
<div class="sourceCode" id="cb1"><html><body><pre class="r">
<span class="no">random_vectors</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="fl">20</span>*<span class="fl">26</span>,<span class="fl">0</span>,<span class="fl">1</span>), <span class="kw">nrow</span><span class="kw">=</span><span class="fl">20</span>, <span class="kw">ncol</span><span class="kw">=</span><span class="fl">26</span>)
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">random_vectors</span>) <span class="kw">&lt;-</span> <span class="no">letters</span>
<span class="no">random_vectors</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span>(<span class="no">random_vectors</span>)</pre></body></html></div>
<p>First, because everything is random, the variables are “ortho-normal in expectation”. That is, we expect that the variables are not correlated with each other. Let’s find out if that is true:</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">random_vectors</span>))</pre></body></html></div>
<p><img src="Lab3_Multiple_regression_II_files/figure-html/unnamed-chunk-3-1.png" width="700"> In general, the distribution of correlations is centered on 0. And, the other “correlations” are all caused by chance (except for the identities, which must be one…<code>a</code> correlated with itself is 1, and so on).</p>
<p>Now, let’s try to predict the values of <code>a</code> from the random vectors in <code>b</code> to <code>z</code>, and see what happens… notice that the total <span class="math inline">\(R^2\)</span> keeps increasing.</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r">
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = a ~ b, data = random_vectors)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;     Min      1Q  Median      3Q     Max </span>
<span class="co">#&gt; -1.7608 -0.8001  0.1766  0.5985  2.0198 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)</span>
<span class="co">#&gt; (Intercept)   0.1156     0.2541   0.455    0.654</span>
<span class="co">#&gt; b             0.1328     0.2697   0.492    0.628</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 0.9935 on 18 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.01329,    Adjusted R-squared:  -0.04152 </span>
<span class="co">#&gt; F-statistic: 0.2425 on 1 and 18 DF,  p-value: 0.6283</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.01329498</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.03284214</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>+<span class="no">d</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.0964248</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>+<span class="no">d</span>+<span class="no">e</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.1272713</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>+<span class="no">d</span>+<span class="no">e</span>+<span class="no">f</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.1278761</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>+<span class="no">d</span>+<span class="no">e</span>+<span class="no">f</span>+<span class="no">g</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.164106</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>+<span class="no">c</span>+<span class="no">d</span>+<span class="no">e</span>+<span class="no">f</span>+<span class="no">g</span>+<span class="no">h</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.1705481</span></pre></body></html></div>
<p>The above example uses <code><a href="https://rdrr.io/r/base/Arithmetic.html">+</a></code> to add new variables in the linear regression formula. This adds each variable as its own predictor and does not add in potential interactions with other variables. We have not discussed the concept of statistical interaction yet in this course, and we do that in coming lectures. Nevertheless, consider what happens to <span class="math inline">\(R^2\)</span> when the <code><a href="https://rdrr.io/r/base/Arithmetic.html">*</a></code> is used in the formula.</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.01329498</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.04374471</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>*<span class="no">d</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.4071319</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>*<span class="no">d</span>*<span class="no">e</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 0.7672118</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>*<span class="no">d</span>*<span class="no">e</span>*<span class="no">f</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))$<span class="no">r.squared</span>
<span class="co">#&gt; [1] 1</span></pre></body></html></div>
<p>The <code><a href="https://rdrr.io/r/base/Arithmetic.html">*</a></code> adds interactions of variables, which effectively increases the number of random predictor variables in this regression. As you can see, with only 5 predictor variables, when we include the potential interactions, this grows to many linearly independent variables (15 in this case):</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>*<span class="no">d</span>*<span class="no">e</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = a ~ b * c * d * e, data = random_vectors)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;        1        2        3        4        5        6        7        8 </span>
<span class="co">#&gt; -0.22332  0.24931  0.36590  1.16874  0.03330 -0.16919 -0.09306 -0.45130 </span>
<span class="co">#&gt;        9       10       11       12       13       14       15       16 </span>
<span class="co">#&gt;  0.17408 -0.21062 -0.46736  0.03612 -0.66952  0.05852  0.36505  0.34736 </span>
<span class="co">#&gt;       17       18       19       20 </span>
<span class="co">#&gt;  0.15800 -0.65354 -0.67464  0.65616 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)</span>
<span class="co">#&gt; (Intercept)  0.38884    1.14346   0.340    0.751</span>
<span class="co">#&gt; b           -0.02818    1.27134  -0.022    0.983</span>
<span class="co">#&gt; c           -0.66783    0.96622  -0.691    0.527</span>
<span class="co">#&gt; d           -0.05423    2.14151  -0.025    0.981</span>
<span class="co">#&gt; e            1.70324    1.55642   1.094    0.335</span>
<span class="co">#&gt; b:c          0.88545    2.46155   0.360    0.737</span>
<span class="co">#&gt; b:d          0.06105    2.23037   0.027    0.979</span>
<span class="co">#&gt; c:d          1.95900    2.94779   0.665    0.543</span>
<span class="co">#&gt; b:e          0.53022    1.84192   0.288    0.788</span>
<span class="co">#&gt; c:e         -1.45522    1.83592  -0.793    0.472</span>
<span class="co">#&gt; d:e         -0.09727    3.21379  -0.030    0.977</span>
<span class="co">#&gt; b:c:d        0.08069    3.09728   0.026    0.980</span>
<span class="co">#&gt; b:c:e       -1.73541    3.62326  -0.479    0.657</span>
<span class="co">#&gt; b:d:e        0.89520    4.29537   0.208    0.845</span>
<span class="co">#&gt; c:d:e        4.48128    5.29820   0.846    0.445</span>
<span class="co">#&gt; b:c:d:e      4.76927    8.66080   0.551    0.611</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 1.024 on 4 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.7672, Adjusted R-squared:  -0.1057 </span>
<span class="co">#&gt; F-statistic: 0.8789 on 15 and 4 DF,  p-value: 0.6237</span></pre></body></html></div>
<p>When you have more predictor variables than you have rows of data in the DV, you will overfit the data because you will have more ways of explaining the data with lines than you have data points. So, when we have 6 predictor variables, and their interactions, the multiple <span class="math inline">\(R^2\)</span> value goes to 1.</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">a</span>~<span class="no">b</span>*<span class="no">c</span>*<span class="no">d</span>*<span class="no">e</span>*<span class="no">f</span>,<span class="kw">data</span><span class="kw">=</span><span class="no">random_vectors</span>))
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = a ~ b * c * d * e * f, data = random_vectors)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt; ALL 20 residuals are 0: no residual degrees of freedom!</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients: (12 not defined because of singularities)</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)</span>
<span class="co">#&gt; (Intercept)   0.3230         NA      NA       NA</span>
<span class="co">#&gt; b             1.3940         NA      NA       NA</span>
<span class="co">#&gt; c            -5.7680         NA      NA       NA</span>
<span class="co">#&gt; d             2.0022         NA      NA       NA</span>
<span class="co">#&gt; e             0.9503         NA      NA       NA</span>
<span class="co">#&gt; f            -0.6628         NA      NA       NA</span>
<span class="co">#&gt; b:c           7.9879         NA      NA       NA</span>
<span class="co">#&gt; b:d           3.5298         NA      NA       NA</span>
<span class="co">#&gt; c:d           9.6906         NA      NA       NA</span>
<span class="co">#&gt; b:e          -8.0869         NA      NA       NA</span>
<span class="co">#&gt; c:e          13.6559         NA      NA       NA</span>
<span class="co">#&gt; d:e          -2.4731         NA      NA       NA</span>
<span class="co">#&gt; b:f           6.4779         NA      NA       NA</span>
<span class="co">#&gt; c:f           9.7582         NA      NA       NA</span>
<span class="co">#&gt; d:f           8.2094         NA      NA       NA</span>
<span class="co">#&gt; e:f           7.6096         NA      NA       NA</span>
<span class="co">#&gt; b:c:d        -8.0890         NA      NA       NA</span>
<span class="co">#&gt; b:c:e        12.5191         NA      NA       NA</span>
<span class="co">#&gt; b:d:e        17.6149         NA      NA       NA</span>
<span class="co">#&gt; c:d:e        11.2012         NA      NA       NA</span>
<span class="co">#&gt; b:c:f             NA         NA      NA       NA</span>
<span class="co">#&gt; b:d:f             NA         NA      NA       NA</span>
<span class="co">#&gt; c:d:f             NA         NA      NA       NA</span>
<span class="co">#&gt; b:e:f             NA         NA      NA       NA</span>
<span class="co">#&gt; c:e:f             NA         NA      NA       NA</span>
<span class="co">#&gt; d:e:f             NA         NA      NA       NA</span>
<span class="co">#&gt; b:c:d:e           NA         NA      NA       NA</span>
<span class="co">#&gt; b:c:d:f           NA         NA      NA       NA</span>
<span class="co">#&gt; b:c:e:f           NA         NA      NA       NA</span>
<span class="co">#&gt; b:d:e:f           NA         NA      NA       NA</span>
<span class="co">#&gt; c:d:e:f           NA         NA      NA       NA</span>
<span class="co">#&gt; b:c:d:e:f         NA         NA      NA       NA</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: NaN on 0 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:      1,  Adjusted R-squared:    NaN </span>
<span class="co">#&gt; F-statistic:   NaN on 19 and 0 DF,  p-value: NA</span></pre></body></html></div>
<p>So, please seriously consider what has been explained here. The DV was generated at random, so there is nothing there to explain in the first place. All of the predictor variables are also generated at random, and shouldn’t explain any variance. Yet, when we use enough random vectors to “explain” the variation in another random vector, we can do so perfectly, explaining 100% of the variance. It is in this sense that we explained everything statistically, without having explained anything at all. These numeric realities are always at play during analyses of real data.</p>
</div>
<div id="review-concept-slamecka-and-orthogonality" class="section level2">
<h2 class="hasAnchor">
<a href="#review-concept-slamecka-and-orthogonality" class="anchor"></a>Review Concept: Slamecka and orthogonality</h2>
<p>In the last lab we discussed <strong>orthogonal</strong> multiple regression. We defined orthogonal as a geometric concept, whereby one dimension is orthogonal to another when they are perpendicular, or connected at a 90 degree angle. Whenever this occurs, it is possible to move along one dimension without also moving along any other dimensions (e.g., you can go back and forth on an X-axis without going anywhere on a Y-axis, so those are orthogonal dimensions). Another way of saying this is that variation along one dimension does not influence variation along another. In other words, dimensions are orthogonal when they are un-correlated with each other.</p>
<p>We also described the <span class="citation">Slamecka (<a href="#ref-slameckaRetroactiveInhibitionConnected1960" role="doc-biblioref">1960</a>)</span> design as an orthogonal design. In chapter 6, we see an example of non-orthogonal multiple regression, where the research design involved non-orthogonal independent variables. Specifically, the IVs are correlated with each other, so they are not orthogonal</p>
<p>Before we discuss that design, it is worth clarifying how it was that the Slamecka design was orthogonal. For example, how exactly is it the case that the IVs in that design were uncorrelated with each other?</p>
<p>Remember that the Slamecka design had two IVs, with three levels each. Original learning had 3 levels (2, 4, &amp; 8); and interpolated learning had 3 levels (0, 4, and 8). At first glance it might seem that these two IVs are highly correlated:</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="no">OL</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">2</span>,<span class="fl">4</span>,<span class="fl">8</span>)
<span class="no">IL</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0</span>,<span class="fl">4</span>,<span class="fl">8</span>)

<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">OL</span>,<span class="no">IL</span>)
<span class="co">#&gt; [1] 0.9819805</span></pre></body></html></div>
<p>The reason that the Slamecka design is an orthogonal design has to do with the <strong>experimental design</strong>, specifically how subjects were assigned to the different levels of each of the independent variables. Depending on how subjects are assigned, the IVs can be correlated (confounded) or uncorrelated.</p>
<p>The lab assignment from week 1 involved writing a table indicating how subjects were assigned to each condition. This is reprinted below:</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">tibble</span>)

<span class="no">slamecka_design</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tribble.html">tribble</a></span>(
  ~<span class="no">Subjects</span>, ~<span class="no">OL</span>, ~<span class="no">IL</span>,
  <span class="co">#--|--|----</span>
  <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">1</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">2</span>, <span class="fl">4</span>, <span class="fl">0</span>,
  <span class="fl">2</span>, <span class="fl">8</span>, <span class="fl">4</span>,
  <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">8</span>,
  <span class="fl">3</span>, <span class="fl">8</span>, <span class="fl">0</span>,
  <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">4</span>,
  <span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">8</span>,
  <span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">4</span>,
  <span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">0</span>,
  <span class="fl">4</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">5</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">5</span>, <span class="fl">2</span>, <span class="fl">8</span>,
  <span class="fl">5</span>, <span class="fl">8</span>, <span class="fl">0</span>,
  <span class="fl">6</span>, <span class="fl">8</span>, <span class="fl">4</span>,
  <span class="fl">6</span>, <span class="fl">4</span>, <span class="fl">8</span>,
  <span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">7</span>, <span class="fl">2</span>, <span class="fl">8</span>,
  <span class="fl">7</span>, <span class="fl">4</span>, <span class="fl">0</span>,
  <span class="fl">7</span>, <span class="fl">8</span>, <span class="fl">4</span>,
  <span class="fl">8</span>, <span class="fl">4</span>, <span class="fl">8</span>,
  <span class="fl">8</span>, <span class="fl">2</span>, <span class="fl">4</span>,
  <span class="fl">8</span>, <span class="fl">8</span>, <span class="fl">0</span>,
  <span class="fl">9</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">9</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">9</span>, <span class="fl">2</span>, <span class="fl">0</span>
)</pre></body></html></div>
<p>Now, if the Subjects, OL, and IL variables are orthogonal, they must all be uncorrelated with each other. We can check this using the <code><a href="https://rdrr.io/r/stats/cor.html">cor()</a></code> function, which will return a matrix of correlations.</p>
<div class="sourceCode" id="cb9"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">slamecka_design</span>)
<span class="co">#&gt;          Subjects OL IL</span>
<span class="co">#&gt; Subjects        1  0  0</span>
<span class="co">#&gt; OL              0  1  0</span>
<span class="co">#&gt; IL              0  0  1</span></pre></body></html></div>
<p>Finally, consider for the moment what a confounded design would look like. For example:</p>
<div class="sourceCode" id="cb10"><html><body><pre class="r"><span class="no">slamecka_confounded</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tribble.html">tribble</a></span>(
  ~<span class="no">Subjects</span>, ~<span class="no">OL</span>, ~<span class="no">IL</span>,
  <span class="co">#--|--|----</span>
  <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">1</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">2</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">2</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">3</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">4</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">5</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">5</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">5</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">6</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">6</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">7</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">7</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">7</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">8</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">8</span>, <span class="fl">2</span>, <span class="fl">0</span>,
  <span class="fl">8</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">9</span>, <span class="fl">8</span>, <span class="fl">8</span>,
  <span class="fl">9</span>, <span class="fl">4</span>, <span class="fl">4</span>,
  <span class="fl">9</span>, <span class="fl">2</span>, <span class="fl">0</span>
)</pre></body></html></div>
<p>I set up the confounded design such that individual subjects were assigned to basically the same levels in OL and IL. As you can see, now the OL and IL variables are perfectly correlated (not orthogonal). They are confounded in the design because if either of them causes some change in the DV, it won’t be clear which one caused the change.</p>
<div class="sourceCode" id="cb11"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">slamecka_confounded</span>)
<span class="co">#&gt;          Subjects        OL        IL</span>
<span class="co">#&gt; Subjects        1 0.0000000 0.0000000</span>
<span class="co">#&gt; OL              0 1.0000000 0.9819805</span>
<span class="co">#&gt; IL              0 0.9819805 1.0000000</span></pre></body></html></div>
<p>Research designs like Slamecka’s <span class="citation">(Slamecka, <a href="#ref-slameckaRetroactiveInhibitionConnected1960" role="doc-biblioref">1960</a>)</span> that employ experimental manipulations of orthogonal variables are, in principle, capable of making causal claims. Specifically, results showing some DV (like recall) clearly changed as a function of the levels of the IVs support a causal claim that something about the IVs caused the change. When we conclude that a manipulation does cause change (and reject the null hypothesis that it was chance causing the change), we are saying that some of the variation in the dependent variable can be “explained” in terms of changes in the IV.</p>
<p>To connect back to my earlier venting about explanation, even in this case the statistical explanation is a very thin kind of explanation. For example, consider a researcher who has no idea about magnetism. They find themselves at a table with a bunch of metal filings. And they move a rock toward and away from the table, and the metal filings don’t move. This rock isn’t magnetic, it doesn’t do anything to the position of the metal filings. The researcher then accidentally grabs a magnet and moves it toward the table. They notice that the metal filings move more and more as the magnet moves closer to the table. It appears to be like magic. The researcher then very systematically measure metal filing displacement as a function of magnet distance, and produces clear results that the the IV (magnet distance) appears to cause metal filings to move. They then publish a paper titled the effect of this weird rock on moving metal filings on my table.</p>
<p>I made up this example to highlight two levels of explanation, statistical and theoretical. At a statistical level the position of the magnet “explains” variance in the displacement of metal filings. However, this level of explanation says nothing about how the cause works, it is not an explanation of magnetism. The statistical level of explanation produces a phenomena or effect (the effect of magnets on moving metal filings) that itself still needs to be explained (e.g., by a theory of magnetism). So, although we take about manipulations in terms of whether they explain our measurements, at the theoretical level manipulations never explain anything. Instead they produce phenomena that require theoretical (an account of how the causes work to produce the phenomena) and not statistical explanation.</p>
</div>
<div id="concept-ii-semi-partial-correlation" class="section level2">
<h2 class="hasAnchor">
<a href="#concept-ii-semi-partial-correlation" class="anchor"></a>Concept II: Semi-partial Correlation</h2>
<p>Above I tried to briefly develop the idea that explanation is hard, even when you are using the best in class tools like orthogonal experimental designs. For example, even when we have a really powerful manipulation like a magnet that can move metal, just running the experiment doesn’t explain why magnetism works, that part requires a theory of magnetism.</p>
<p>In non-experimental designs the prospect of explanation is even more remote and in my opinion extremely difficult. For example, in correlational research, a typical strategy is to take many measurements and then see what correlates with what. This strategy can find patterns in data, but as we saw in the first concept section it is possible to “explain” variance with completely random vectors too.</p>
<p>In Chapter 6, <span class="citation">Abdi et al. (<a href="#ref-abdiExperimentalDesignAnalysis2009" role="doc-biblioref">2009</a>)</span> contrasts orthogonal multiple regression suitable for experimental designs with non-orthogonal multiple regression typical of correlation research. In both cases the research design has multiple variables that could “explain” variance in some dependent measure. Most measures of human behavior are complex and multiply determined, so it is natural consider the idea that different variables may or may not have a causal influence on the measure, and if they do cause change, then each variable may cause different amounts of change. As a result, when there are multiple variables, researchers might be interested in figuring out, for each variable, how much it causes change in the measure.</p>
<p>In the case of experimental research with orthogonal designs, the determination of the action of causal variables is done by independently manipulating a variable of interest, and measuring whether the manipulation has an effect on the measure. In the case of non-orthogonal correlational research it is not possible make any causal claims. However, it is possible to add variables of interest into a regression, and this will raise many more questions about possible causes. Let’s consider some of the issues by looking at the textbook example.</p>
<div id="age-speech-rate-and-memory" class="section level3">
<h3 class="hasAnchor">
<a href="#age-speech-rate-and-memory" class="anchor"></a>Age, speech rate, and memory</h3>
<p><span class="citation">Abdi et al. (<a href="#ref-abdiExperimentalDesignAnalysis2009" role="doc-biblioref">2009</a>)</span> discusses a research design by <span class="citation">Hulme et al. (<a href="#ref-hulmeSpeechRateDevelopment1984" role="doc-biblioref">1984</a>)</span> who were interested in whether the variables of age and speech rate were involve in memory ability. The basic claim was that older children who can talk faster will have better memory abilities compared to younger children who talk slower, perhaps because faster speech rate would help one rehearse more information.</p>
<p>It is not possible to randomly assign people to different ages or different speech rate abilities, so this kind of research can not employ an experimental design with orthogonal variables. As a result, the “Independent Variables” of Age (X) and speech rate (T) are not independent from each other, they are confounded and correlated with one another. For example, as children age they also talk faster, so the age variable is positively correlated with talking faster.</p>
<p>Example data from the textbook is below:</p>
<div class="sourceCode" id="cb12"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">tibble</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">dplyr</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">ggplot2</span>)

<span class="no">data</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span>(<span class="kw">X</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">4</span>,<span class="fl">4</span>,<span class="fl">7</span>,<span class="fl">7</span>,<span class="fl">10</span>,<span class="fl">10</span>),
               <span class="kw">T</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">2</span>,<span class="fl">4</span>,<span class="fl">3</span>,<span class="fl">6</span>),
               <span class="kw">Y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">14</span>,<span class="fl">23</span>,<span class="fl">30</span>,<span class="fl">50</span>,<span class="fl">39</span>,<span class="fl">67</span>))</pre></body></html></div>
<p>We can run a multiple linear regression to ask the question, how much does Age (X) and Speech Rate (T) explain variance in memory ability (Y). In this fake date, we see that <span class="math inline">\(R^2\)</span> is nearly one, so taken togetehr, X and T seem to ‘explain’ almost all the variation in Y.</p>
<div class="sourceCode" id="cb13"><html><body><pre class="r"><span class="co"># Predict Y as a function of X and T</span>
(<span class="no">overall_model</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">Y</span>~<span class="no">X</span>+<span class="no">T</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">data</span>)))
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = Y ~ X + T, data = data)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      1      2      3      4      5      6 </span>
<span class="co">#&gt; -1.167 -1.667  2.333  3.333 -1.167 -1.667 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   </span>
<span class="co">#&gt; (Intercept)    1.667      3.598   0.463  0.67470   </span>
<span class="co">#&gt; X              1.000      0.725   1.379  0.26162   </span>
<span class="co">#&gt; T              9.500      1.087   8.736  0.00316 **</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 2.877 on 3 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.9866, Adjusted R-squared:  0.9776 </span>
<span class="co">#&gt; F-statistic: 110.1 on 2 and 3 DF,  p-value: 0.001559</span></pre></body></html></div>
<p>We can also look at the correlations and <span class="math inline">\(R^2\)</span> values between all the variables.</p>
<div class="sourceCode" id="cb14"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">data</span>)
<span class="co">#&gt;           X         T         Y</span>
<span class="co">#&gt; X 1.0000000 0.7500000 0.8027961</span>
<span class="co">#&gt; T 0.7500000 1.0000000 0.9889517</span>
<span class="co">#&gt; Y 0.8027961 0.9889517 1.0000000</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="no">data</span>)^<span class="fl">2</span>
<span class="co">#&gt;           X         T         Y</span>
<span class="co">#&gt; X 1.0000000 0.5625000 0.6444815</span>
<span class="co">#&gt; T 0.5625000 1.0000000 0.9780254</span>
<span class="co">#&gt; Y 0.6444815 0.9780254 1.0000000</span></pre></body></html></div>
<p>Here we see that X explains .64 of the variation in Y, and T explains .97 of the variation in Y. Notice this adds up to .97+.64 = 1.61, which is greater than 1. In other words we are explaining more than 100% of the variation, which is nonsensical. How could this happen?</p>
<p>The assumption is that X and T are correlated with each other, and we can see in the correlation matrix that this is true, the correlation is <span class="math inline">\(r=.75\)</span>. As a result, we consider the idea that there are three parts of the puzzle. The unique potential influence of X, the unique potential influence T, and the shared part between X and T. In other words, part of the age and speech rate variables are measuring the same underlying thing (we call this the shared part), but they are also each measuring some unique things.</p>
<p>The <span class="math inline">\(R^2\)</span> for X and Y is .64, and this includes the unique part of X and the shared part with T. The <span class="math inline">\(R^2\)</span> for T and Y is .97 and includes the unique part of T, and the shared part with X. If we add the two together, we add the shared part twice, which causes the total <span class="math inline">\(R^2\)</span> to be greater than 1. <strong>Using the technique of semi-partial correlation, it is possible to re-express the two confounded variables into three sources of variation, the unique parts of each, and the shared part.</strong></p>
<p>Semi-partial correlation involves a process of “de-correlation”, or “taking the line out the data”. Let’s take a closer look at the de-correlation process before using it for semi-partial correlation.</p>
</div>
<div id="the-residuals-are-de-correlated" class="section level3">
<h3 class="hasAnchor">
<a href="#the-residuals-are-de-correlated" class="anchor"></a>The residuals are de-correlated</h3>
<p>Consider how much X (Age) explains variation in Y (memory ability). We conduct the regression, then we look at two components of the Y data expressed in terms X. These are the parts of Y that fall on the regression line (the predicted parts), and the parts of Y that are not on the line (the residual error).</p>
<div class="sourceCode" id="cb15"><html><body><pre class="r"><span class="co"># Predict Y as a function of X</span>

<span class="no">lm.x</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">Y</span>~<span class="no">X</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">data</span>)

<span class="no">data</span> <span class="kw">&lt;-</span> <span class="no">data</span> <span class="kw">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(<span class="kw">X_residuals</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.x</span>),
         <span class="kw">X_predicted_Y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">lm.x</span>))

<span class="kw pkg">knitr</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html">kable</a></span>(<span class="no">data</span>)</pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="right">X</th>
<th align="right">T</th>
<th align="right">Y</th>
<th align="right">X_residuals</th>
<th align="right">X_predicted_Y</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">14</td>
<td align="right">-5.916667</td>
<td align="right">19.91667</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">2</td>
<td align="right">23</td>
<td align="right">3.083333</td>
<td align="right">19.91667</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">2</td>
<td align="right">30</td>
<td align="right">-7.166667</td>
<td align="right">37.16667</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">4</td>
<td align="right">50</td>
<td align="right">12.833333</td>
<td align="right">37.16667</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">3</td>
<td align="right">39</td>
<td align="right">-15.416667</td>
<td align="right">54.41667</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">6</td>
<td align="right">67</td>
<td align="right">12.583333</td>
<td align="right">54.41667</td>
</tr>
</tbody>
</table>
<p>Now, the <code>data</code> table contains three versions of Y, the original Y variable decomposed into the predicted part (regression line part), and the residual part. Let’s plot all of these and run a regression line through them.</p>
<div class="sourceCode" id="cb16"><html><body><pre class="r">
<span class="no">A</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">Y</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">X</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>, <span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="no">B</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">X_predicted_Y</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">X</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>,<span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="no">C</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">X_residuals</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">X</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>,<span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">patchwork</span>)

<span class="no">A</span>+<span class="no">B</span>+<span class="no">C</span></pre></body></html></div>
<p><img src="Lab3_Multiple_regression_II_files/figure-html/unnamed-chunk-17-1.png" width="700"></p>
<p>Notice that the residuals have a flat line. The process of decomposing Y into one part that is on the regression line, and a residual part reflecting deviation or error from the line, is a process of <strong>de-correlation</strong>. Specifically, the residual part of X is the part that does not correlate with Y. In other words, the residuals are what the pattern looks like when the correlation has been subtracted out. When you subtract out the linear correlation, the leftover part by definition has no correlation left, so the residuals are forced to be statistically independent from the dependent measure.</p>
<p>We can show the same thing using the other predictor variable T. If we predict Y from T, then we can plot the residuals and show that they too have the correlation removed.</p>
<div class="sourceCode" id="cb17"><html><body><pre class="r">
<span class="co"># Predict Y as a function of T</span>

<span class="no">lm.t</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">Y</span>~<span class="no">T</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">data</span>)

<span class="no">data</span> <span class="kw">&lt;-</span> <span class="no">data</span> <span class="kw">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(<span class="kw">T_residuals</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.t</span>),
         <span class="kw">T_predicted_Y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">lm.t</span>))
<span class="no">D</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">Y</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">T</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>, <span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="no">E</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">T_predicted_Y</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">T</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>,<span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="no">F</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="no">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">y</span><span class="kw">=</span><span class="no">T_residuals</span>, <span class="kw">x</span><span class="kw">=</span><span class="no">T</span>))+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span>()+
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span>(<span class="kw">method</span><span class="kw">=</span><span class="st">"lm"</span>,<span class="kw">se</span><span class="kw">=</span><span class="fl">FALSE</span>)

<span class="no">D</span>+<span class="no">E</span>+<span class="no">F</span></pre></body></html></div>
<p><img src="Lab3_Multiple_regression_II_files/figure-html/unnamed-chunk-18-1.png" width="700"></p>
</div>
<div id="semi-partial-correlation" class="section level3">
<h3 class="hasAnchor">
<a href="#semi-partial-correlation" class="anchor"></a>Semi-partial correlation</h3>
<p>The technique of semi-partial correlation relies on the decorrelation process that we just reviewed. The idea is that our predictor variables X and T each have unique variance, but they also have shared variance. We want to separate these three sources of variance into <span class="math inline">\(X_{\text{unique}}\)</span>, <span class="math inline">\(T_{\text{unique}}\)</span>, and <span class="math inline">\(XT_{\text{shared}}\)</span>.</p>
</div>
<div id="finding-the-unique-part-of-x" class="section level3">
<h3 class="hasAnchor">
<a href="#finding-the-unique-part-of-x" class="anchor"></a>Finding the unique part of X</h3>
<p>What part of X is unique to X and not correlated with T? Let’s consider X as the dependent variable, and regress T onto it as the predictor variable.</p>
<div class="sourceCode" id="cb18"><html><body><pre class="r"><span class="co">## Semi-partial correlation</span>
<span class="co"># Part uniquely explained by X</span>
<span class="no">lm.xt</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">X</span>~<span class="no">T</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">data</span>)</pre></body></html></div>
<p>Now, we know that the residuals here are the part of X that is not linearly correlated with T.</p>
<div class="sourceCode" id="cb19"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.xt</span>)
<span class="co">#&gt;      1      2      3      4      5      6 </span>
<span class="co">#&gt; -0.750 -1.875  1.125 -1.125  3.000 -0.375</span></pre></body></html></div>
<p>In other words, these values are <span class="math inline">\(X_{\text{unique}}\)</span> with the <span class="math inline">\(XT_{\text{shared}}\)</span> part removed. Remember, the original question was to figure out how much X alone explained Y. We can do this by computing the <span class="math inline">\(R^2\)</span> between this vector of residuals, and the Y dependent measure.</p>
<div class="sourceCode" id="cb20"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.xt</span>),<span class="no">data</span>$<span class="no">Y</span>)^<span class="fl">2</span>
<span class="co">#&gt; [1] 0.008528111</span></pre></body></html></div>
</div>
<div id="finding-the-unique-part-of-t" class="section level3">
<h3 class="hasAnchor">
<a href="#finding-the-unique-part-of-t" class="anchor"></a>Finding the unique part of T</h3>
<p>We repeat the process to find <span class="math inline">\(T_{\text{unique}}\)</span>, except now we regress X as the predictor onto T as the dependent variable.</p>
<div class="sourceCode" id="cb21"><html><body><pre class="r"><span class="co"># Part uniquely explained by T</span>
<span class="no">lm.tx</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">T</span>~<span class="no">X</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">data</span>)
<span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.tx</span>)
<span class="co">#&gt;    1    2    3    4    5    6 </span>
<span class="co">#&gt; -0.5  0.5 -1.0  1.0 -1.5  1.5</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.tx</span>),<span class="no">data</span>$<span class="no">Y</span>)^<span class="fl">2</span>
<span class="co">#&gt; [1] 0.342072</span></pre></body></html></div>
</div>
<div id="the-shared-part" class="section level3">
<h3 class="hasAnchor">
<a href="#the-shared-part" class="anchor"></a>The shared part</h3>
<p>To estimate how much variance the shared part of X and T explain we can do a bit algebra. We take the total multiple <span class="math inline">\(R^2\)</span> from the original model <code><a href="https://rdrr.io/r/stats/lm.html">lm(Y~X+T)</a></code>, and then subtract <span class="math inline">\(X_{\text{unique}}\)</span> and <span class="math inline">\(T_{\text{unique}}\)</span> from the total.</p>
<div class="sourceCode" id="cb22"><html><body><pre class="r"><span class="co"># Part common to X and T</span>
<span class="no">overall_model</span>$<span class="no">r.squared</span> - <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.xt</span>),<span class="no">data</span>$<span class="no">Y</span>)^<span class="fl">2</span> - <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span>(<span class="no">lm.tx</span>),<span class="no">data</span>$<span class="no">Y</span>)^<span class="fl">2</span>
<span class="co">#&gt; [1] 0.6359534</span></pre></body></html></div>
</div>
<div id="what-has-been-explained" class="section level3">
<h3 class="hasAnchor">
<a href="#what-has-been-explained" class="anchor"></a>What has been explained?</h3>
<p>Semi-partial correlation is a useful demonstration of the idea of partitioning sources of variance into unique and shared portions. However, if we returned to the question of what has been explained, we will likely have more questions than we had in the first place.</p>
<p>Apparently the unique part of Age (X) only explained a small part of the variation in memory ability (.0085). The Age variable is not a clear measure in the first place, as it is a proxy variable for a huge amount of things that change over development. The speech rate variable is more specific than the age variable, but it explains less variance (.34) than the shared part between age and speech rate (.63). What is the share part actually referring to? Who knows? It’s a latent construct that in some sense is not related to age or speech rate. And, because the design is correlational, it is not clear if the the things that cause memory ability are causing this latent construct to change or vice-versa. Explanation is hard, and non-orthogonal designs can easily raise more questions than they answer. Partly for this reason, we stick to examining experimental designs for the remainder of this course.</p>
</div>
</div>
<div id="practical-i-semi-partial-correlation-with-ppcor" class="section level2">
<h2 class="hasAnchor">
<a href="#practical-i-semi-partial-correlation-with-ppcor" class="anchor"></a>Practical I: semi-partial correlation with <code>ppcor</code>
</h2>
<p>The <code>ppcor</code> package has a function for computing semi-partial correlations. We can see that the <code><a href="https://rdrr.io/pkg/ppcor/man/spcor.html">spcor()</a></code> function returns the same values as in the textbook book example.</p>
<div class="sourceCode" id="cb23"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">ppcor</span>)

<span class="no">data</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span>(<span class="kw">X</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">4</span>,<span class="fl">4</span>,<span class="fl">7</span>,<span class="fl">7</span>,<span class="fl">10</span>,<span class="fl">10</span>),
               <span class="kw">T</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">2</span>,<span class="fl">4</span>,<span class="fl">3</span>,<span class="fl">6</span>),
               <span class="kw">Y</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">14</span>,<span class="fl">23</span>,<span class="fl">30</span>,<span class="fl">50</span>,<span class="fl">39</span>,<span class="fl">67</span>))

<span class="fu"><a href="https://rdrr.io/pkg/ppcor/man/spcor.html">spcor</a></span>(<span class="no">data</span>, <span class="kw">method</span> <span class="kw">=</span> <span class="st">"pearson"</span>)
<span class="co">#&gt; $estimate</span>
<span class="co">#&gt;             X          T         Y</span>
<span class="co">#&gt; X  1.00000000 -0.2963241 0.4120552</span>
<span class="co">#&gt; T -0.07367089  1.0000000 0.6488088</span>
<span class="co">#&gt; Y  0.09234777  0.5848692 1.0000000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $p.value</span>
<span class="co">#&gt;           X         T         Y</span>
<span class="co">#&gt; X 0.0000000 0.6283051 0.4906046</span>
<span class="co">#&gt; T 0.9062842 0.0000000 0.2362282</span>
<span class="co">#&gt; Y 0.8825865 0.3002769 0.0000000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $statistic</span>
<span class="co">#&gt;            X          T         Y</span>
<span class="co">#&gt; X  0.0000000 -0.5373837 0.7832889</span>
<span class="co">#&gt; T -0.1279494  0.0000000 1.4767956</span>
<span class="co">#&gt; Y  0.1606375  1.2489073 0.0000000</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $n</span>
<span class="co">#&gt; [1] 6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $gp</span>
<span class="co">#&gt; [1] 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $method</span>
<span class="co">#&gt; [1] "pearson"</span></pre></body></html></div>
<p>Note that <code><a href="https://rdrr.io/pkg/ppcor/man/spcor.html">spcor()</a></code> returns correlations (<span class="math inline">\(r\)</span> values). Square them to produce coefficients of determination (<span class="math inline">\(R^2\)</span>).</p>
<div class="sourceCode" id="cb24"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/ppcor/man/spcor.html">spcor</a></span>(<span class="no">data</span>[], <span class="kw">method</span> <span class="kw">=</span> <span class="st">"pearson"</span>)$<span class="no">estimate</span>^<span class="fl">2</span>
<span class="co">#&gt;             X          T         Y</span>
<span class="co">#&gt; X 1.000000000 0.08780798 0.1697895</span>
<span class="co">#&gt; T 0.005427400 1.00000000 0.4209528</span>
<span class="co">#&gt; Y 0.008528111 0.34207202 1.0000000</span></pre></body></html></div>
</div>
<div id="lab-3-generalization-assignment" class="section level2">
<h2 class="hasAnchor">
<a href="#lab-3-generalization-assignment" class="anchor"></a>Lab 3 Generalization Assignment</h2>
<div id="instructions" class="section level3">
<h3 class="hasAnchor">
<a href="#instructions" class="anchor"></a>Instructions</h3>
<p>Your assignment instructions are the following:</p>
<ol style="list-style-type: decimal">
<li>Work inside the new R project for stats II that you created</li>
<li>Create a new R Markdown document called “Lab3.Rmd”</li>
<li>Use Lab3.Rmd to show your work attempting to solve the following generalization problems. Commit your work regularly so that it appears on your Github repository.</li>
<li>Submit your github repository link for Lab 3 on blackboard.</li>
</ol>
</div>
<div id="problems" class="section level3">
<h3 class="hasAnchor">
<a href="#problems" class="anchor"></a>Problems</h3>
<ol style="list-style-type: decimal">
<li>The problem for this week is to develop a little bit of content (tutorial material) that could be added to this lab. (6 points)</li>
</ol>
<p>As I mentioned in lab, I ran out of time a little bit this week, so I didn’t get a chance to provide lots of examples of doing semi-partial correlation in R. I also didn’t cover everything in Chapter 6, for example, I didn’t talk about the difference between partial and semi-partial correlation. On the whole, I am assuming that there are many ways the tutorial content in this lab could be improved.</p>
<p>The purpose of this lab is for you to try and create something that could in principle be added to the content of this lab. For example, maybe you think there is another way to explain semi-partial correlation, or the concept of orthogonality, or you would like to provide a different example computing semi-partial correlation.</p>
<p>This is a fairly open lab assignment. Your task is to develop a little tutorial or example that could be inserted into this lab. I’m not going to give guidelines on how long this should be. It probably shouldn’t be as long as an entire lab, and it probably shouldn’t be as short as 1 line of code and one sentence. I’m thinking a paragraph or two of explanation, and a code snippet or two to illustrate your example. Imagine you are trying to explain these concepts or practical tips, and see what you come up with.</p>
<p>When you submit your assignment <strong>please let me know if you are OK with me putting your example up on this course website for this lab.</strong> It is totally OK if you do not want to do this. If you are OK with this, then I will add a new section to this lab called “Student contributed examples”; and, you will be recognized as a contributing author to this lab. Note also, that I am sharing this lab on a creative commons license (CC BY SA 4.0), which basically means that other people can copy, edit, and re-use this material as they wish, but they do have to credit the content creators if they do this. If you choose to have your example displayed, then your content will also be on this license. We can discuss the license in more detail next week if you have questions.</p>
<p>Good luck!</p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-abdiExperimentalDesignAnalysis2009">
<p>Abdi, H., Edelman, B., Dowling, W. J., &amp; Valentin, D. (2009). <em>Experimental design and analysis for psychology</em>. Oxford University Press.</p>
</div>
<div id="ref-hulmeSpeechRateDevelopment1984">
<p>Hulme, C., Thomson, N., Muir, C., &amp; Lawrence, A. (1984). Speech rate and the development of short-term memory span. <em>Journal of Experimental Child Psychology</em>, <em>38</em>(2), 241–253. <a href="https://doi.org/10/cs58jc">https://doi.org/10/cs58jc</a></p>
</div>
<div id="ref-slameckaRetroactiveInhibitionConnected1960">
<p>Slamecka, N. J. (1960). Retroactive inhibition of connected discourse as a function of practice level. <em>Journal of Experimental Psychology</em>, <em>59</em>(2), 104–108. <a href="https://doi.org/10/b5z2nt">https://doi.org/10/b5z2nt</a></p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by <a href="https://crumplab.github.io">Matthew Crump</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
